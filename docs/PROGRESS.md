# Progress

This file tracks completed changes and upcoming features for the project.

## [2025-10] Phase 1: Web UI and Knowledge Base Enhancement

### Added (Core Modules)
- **config.py**: Unified configuration system with `.env` support
  - Path configuration (Chroma DB, CVE feeds, temp uploads)
  - Model names (Llama, SentenceTransformer)
  - Default parameters (speed, mode, schema, precision)
  - RAG configuration (history length, retrieval top-k, chunk settings)
  - Web UI configuration (port, share, server name)
  - Validation functions and debug utilities
- **core/models.py**: Llama model loading and management
  - LlamaModel class with FP16, SDPA, low memory support
  - Backward compatible utility functions
  - Automatic device selection
  - cleanup() for resource management
- **core/embeddings.py**: SentenceTransformer wrapper
  - EmbeddingModel class with batch encoding
  - retrieve_top_k() for similarity search
  - Precision conversion (float32/float16)
  - Backward compatible utility functions
- **core/chroma_manager.py**: Chroma vector database CRUD
  - ChromaManager class for database operations
  - add_documents() with metadata support
  - query() with filtering
  - delete_by_source() for source removal
  - get_stats() and list_sources() for management
  - Metadata schema: source_type, source_name, added_date, chunk_index, precision
- **core/cve_lookup.py**: CVE JSON parsing (V4/V5)
  - lookup_cve() for single CVE lookup
  - batch_lookup_cves() for multiple CVEs
  - extract_cves_regex() for regex extraction
  - format_cve_description() for output formatting
- **core/pdf_processor.py**: PDF text extraction
  - PDFProcessor class with PyMuPDF
  - extract_text() and extract_text_by_pages()
  - extract_cve_context() for context windows

### Added (RAG Implementation)
- **rag/pure_python.py**: Conversation-aware RAG (Phase 1)
  - ConversationHistory class (fixed-size deque, last 10 rounds)
  - PureRAG class with full RAG workflow
  - query() with history-aware context retrieval
  - summarize_report() for executive summaries
  - validate_cve_usage() for CVE correctness checking
  - answer_question_about_report() for Q&A
  - process_report_for_cve_validation() for PDF + CVE lookup

### Added (Tools and Web UI)
- **add_to_embeddings.py**: Incremental knowledge base updates
  - Add PDFs: `--source=pdf --files=report.pdf,report2.pdf`
  - Add CVE data: `--source=cve --year=2024 --schema=v5`
  - Configurable chunk size and batch size
  - Automatic metadata tagging
  - Progress bars and error handling
- **web/web_ui.py**: Gradio web interface (Phase 1)
  - Claude Projects-style layout (left chat + right settings/KB)
  - Conversational AI with 10-round history
  - Upload PDF for validation (summarize/validate/add to KB)
  - Knowledge base management (add/view/refresh)
  - Real-time statistics display
  - Analysis settings (speed/mode/schema dropdowns)
  - Auto-launch browser on startup

### Changed
- **.gitignore**: Added `.env` and `temp_uploads/` exclusions
- **requirements.txt**: Added `python-dotenv` and `gradio`
- Repository structure: Added `core/`, `rag/`, `web/` directories

### Repository Structure (Updated)
```
RAG_LLM_CVE/
â”œâ”€â”€ core/                  # Shared modules (Phase 1)
â”‚   â”œâ”€â”€ models.py          # Llama loading
â”‚   â”œâ”€â”€ embeddings.py      # SentenceTransformer
â”‚   â”œâ”€â”€ chroma_manager.py  # Chroma CRUD
â”‚   â”œâ”€â”€ cve_lookup.py      # CVE parsing
â”‚   â””â”€â”€ pdf_processor.py   # PDF extraction
â”œâ”€â”€ rag/                   # RAG implementations
â”‚   â””â”€â”€ pure_python.py     # Phase 1: Pure Python RAG
â”œâ”€â”€ web/                   # Web interfaces
â”‚   â””â”€â”€ webUI.py           # Phase 1: Gradio UI
â”œâ”€â”€ validate_report.py     # CLI application (original)
â”œâ”€â”€ build_embeddings.py    # Generate embeddings
â”œâ”€â”€ add_to_embeddings.py   # Incremental updates (new)
â”œâ”€â”€ extract_cve.py         # Export CVE descriptions
â”œâ”€â”€ config.py              # Configuration loader (new)
â”œâ”€â”€ .env.example           # Config template (new)
â”œâ”€â”€ .env                   # Local config (gitignored)
â”œâ”€â”€ FEATURE_PLAN.md        # Planning document (new)
â”œâ”€â”€ CLAUDE.md              # User guide
â”œâ”€â”€ ARCHITECTURE.md        # Technical details
â””â”€â”€ PROGRESS.md            # This file
```

### Usage Examples
```bash
# Web UI (recommended for demos)
python web/web_ui.py

# Add PDFs to knowledge base
python cli/add_to_embeddings.py --source=pdf --files=report1.pdf,report2.pdf

# Add CVE data to knowledge base
python cli/add_to_embeddings.py --source=cve --year=2024 --schema=v5

# Original CLI (still works)
python cli/validate_report.py --speed=fast --extension=chroma
```

### Phase 1 Goals Achieved
âœ… Core modules created and tested (all imports successful)
âœ… Configuration system with .env support
âœ… Incremental knowledge base updates (PDFs and CVE data)
âœ… Conversation-aware RAG implementation
âœ… Gradio web UI with Claude Projects-style layout
âœ… Knowledge base management (add/view sources)
âœ… Real-time statistics and refresh

## [2025-10] Phase 3: Web UI Improvements

### Changed (Web UI Polish)
- **Upload status simplification** (web_ui.py and web_ui_langchain.py):
  - Removed instructional text from success messages ("Type 'summarize' or 'validate' in chat")
  - Success display now shows only: filename + "âœ… Ready"
  - Error display now shows only: filename + "âŒ Upload Error" (no error details)
  - Safe filename extraction with try-catch fallback
- **Empty container handling**:
  - Added JavaScript MutationObserver to detect and hide empty HTML containers
  - Prevents visible horizontal lines when upload status is cleared
  - Auto-detects `.html-container` with empty `.prose` child and applies `display: none`
- **UI consistency** (web_ui_langchain.py):
  - Removed LangChain mentions from UI labels (except main title)
  - "Conversation (LangChain)" â†’ "Conversation"
  - "Chat History (Auto-managed by LangChain Memory)" â†’ "Chat History"
  - Maintains consistent user experience across Phase 1 and Phase 2

### Technical Details
- **MutationObserver pattern**: Watches DOM changes in real-time to hide empty containers
- **Gradio HTML filtering**: Inline `<script>` tags are filtered, requiring global JavaScript injection via `demo.load(..., js=...)`
- **Status block design**: Colored notification boxes with left border (green=success, red=error, yellow=uploading)
- **Error handling**: Graceful fallback to generic "File" label if filename extraction fails

### User Experience Impact
- Cleaner, less cluttered upload status display
- No confusing instructional text that might distract users
- No visible layout artifacts when upload status is cleared
- Consistent branding between Phase 1 and Phase 2 implementations

## [2025-10] Phase 2: LangChain Implementation

### Added (LangChain RAG)
- **rag/langchain_impl.py**: LangChain-based RAG system
  - LangChainRAG class with automatic initialization
  - HuggingFacePipeline wrapper for Llama model
  - HuggingFaceEmbeddings for SentenceTransformer
  - ConversationBufferWindowMemory (k=10 rounds, automatic management)
  - ConversationalRetrievalChain for standardized RAG workflow
  - Chroma vectorstore integration via LangChain
  - query() with automatic conversation history
  - summarize_report(), validate_cve_usage(), answer_question_about_report()
  - process_report_for_cve_validation() for PDF processing
  - add_document_to_kb() with automatic embedding generation
  - get_kb_stats() and delete_source() for KB management

### Added (LangChain Web UI)
- **web/web_ui_langchain.py**: Gradio interface using LangChain
  - Same Claude Projects-style layout as Phase 1
  - Uses LangChainRAG for backend
  - Automatic memory management (no manual history tracking)
  - Port 7861 (Phase 1 uses 7860) - both can run simultaneously
  - Clear labeling as "LangChain" version

### Key Differences: Phase 1 vs Phase 2

| Feature | Phase 1 (Pure Python) | Phase 2 (LangChain) |
|---------|----------------------|---------------------|
| **Conversation History** | Manual (deque with sliding window) | Automatic (ConversationBufferWindowMemory) |
| **RAG Workflow** | Custom query + retrieval logic | ConversationalRetrievalChain |
| **Embeddings** | Direct SentenceTransformer usage | HuggingFaceEmbeddings abstraction |
| **Vector Store** | Direct Chroma client queries | LangChain Chroma wrapper |
| **Code Complexity** | Lower-level control, more code | Higher-level abstraction, less code |
| **Flexibility** | Full control over every step | Standardized patterns |
| **Learning Curve** | Understand RAG internals | Understand LangChain APIs |
| **Port** | 7860 | 7861 |

### Usage Examples
```bash
# Phase 1 (Pure Python, port 7860)
python web/web_ui.py

# Phase 2 (LangChain, port 7861)
python web/web_ui_langchain.py

# Both can run simultaneously for A/B comparison
```

### Repository Structure (Final)
```
RAG_LLM_CVE/
â”œâ”€â”€ core/                  # Shared modules
â”‚   â”œâ”€â”€ models.py          # Llama loading
â”‚   â”œâ”€â”€ embeddings.py      # SentenceTransformer
â”‚   â”œâ”€â”€ chroma_manager.py  # Chroma CRUD
â”‚   â”œâ”€â”€ cve_lookup.py      # CVE parsing
â”‚   â””â”€â”€ pdf_processor.py   # PDF extraction
â”œâ”€â”€ rag/                   # RAG implementations
â”‚   â”œâ”€â”€ pure_python.py     # Phase 1: Manual implementation
â”‚   â””â”€â”€ langchain_impl.py  # Phase 2: LangChain (new)
â”œâ”€â”€ web/                   # Web interfaces
â”‚   â”œâ”€â”€ web_ui.py          # Phase 1: Pure Python
â”‚   â””â”€â”€ web_ui_langchain.py # Phase 2: LangChain (new)
â”œâ”€â”€ validate_report.py     # CLI application (original)
â”œâ”€â”€ build_embeddings.py    # Generate embeddings
â”œâ”€â”€ add_to_embeddings.py   # Incremental updates
â”œâ”€â”€ extract_cve.py         # Export CVE descriptions
â”œâ”€â”€ config.py              # Configuration loader
â”œâ”€â”€ .env.example           # Config template
â””â”€â”€ FEATURE_PLAN.md        # Planning document
```

### Phase 2 Goals Achieved
âœ… LangChain RAG implementation with chains and memory
âœ… ConversationalRetrievalChain integration
âœ… Automatic conversation history management
âœ… Gradio web UI using LangChain backend
âœ… Coexistence with Phase 1 (both can run simultaneously)
âœ… Standardized LangChain patterns throughout
âœ… All imports tested and working

### Performance Comparison (To Be Tested)
- [ ] Memory usage: Phase 1 vs Phase 2
- [ ] Response latency: Phase 1 vs Phase 2
- [ ] Conversation quality: Phase 1 vs Phase 2
- [ ] Code maintainability assessment

### Recommendations
- **Phase 1 (pure_python.py)**: Best for learning RAG internals, maximum control
- **Phase 2 (langchain_impl.py)**: Best for production, standardized patterns, community support
- **Both**: Keep both for comparison and fallback options

## [2025-01] Chunk Overlap Implementation

### Added (Configuration)
- **CHUNK_OVERLAP_RATIO** environment variable in `.env` and `.env.example`
  - Type: float (0.0 - 1.0)
  - Default: 0.3 (30% overlap)
  - Applies to PDF documents only; CVE data uses 0.0 (no overlap)
  - Example: CHUNK_SIZE=10 with CHUNK_OVERLAP_RATIO=0.3 results in 3-sentence overlap
- **config.py** updated to load `CHUNK_OVERLAP_RATIO`
  - Validates as float type
  - Displays in `print_config()` debug output

### Added (build_embeddings.py)
- **split_with_overlap()** function for overlapping chunks
  - Configurable overlap ratio (default: 0.3)
  - Step size calculation: `step_size = max(1, int(chunk_size * (1 - overlap_ratio)))`
  - Prevents context loss at chunk boundaries
  - Comprehensive docstring with examples
- **Updated process_pdf()** to use `split_with_overlap()`
  - Displays overlap configuration: "Chunk overlap: 30.0% (3 sentences)"
  - Preserves backward compatibility (overlap_ratio=0.0 behaves like split())
- **Preserved split()** for CVE data (no overlap needed for atomic descriptions)

### Added (add_to_embeddings.py)
- **split_list_with_overlap()** function mirroring build_embeddings.py
- **Updated process_pdf_files()** to use overlap for incremental PDF additions
- Consistent overlap behavior across all PDF processing tools

### Performance Impact
- **Chunk count increase**: +40% with 30% overlap (e.g., 50 â†’ 71 chunks for 500 sentences)
- **Storage increase**: +40% embedding storage
- **Generation time**: +40% embedding generation time
- **Retrieval accuracy**: +13-17% improvement (based on ARCHITECTURE.md analysis)

### Design Rationale
- **30% overlap chosen as default**: Best ROI (88% accuracy vs 75% with no overlap)
- **50% overlap**: Only +4% accuracy improvement but +100% storage cost (poor ROI)
- **CVE data exemption**: Atomic descriptions don't benefit from overlap
- **Configurable**: Users can adjust via `.env` for specific needs

### Verified Testing
âœ… **Configuration loading** (2025-01-18):
- `config.py` correctly loads `CHUNK_OVERLAP_RATIO: 0.3`
- Displays in debug output

âœ… **Overlap functionality** (2025-01-18):
- Test with 30 sentences, chunk_size=10, overlap_ratio=0.3
- Generated 4 chunks with correct 3-sentence overlap:
  - Chunk 0: Sentences 1-10
  - Chunk 1: Sentences 8-17 (3 overlap)
  - Chunk 2: Sentences 15-24 (3 overlap)
  - Chunk 3: Sentences 22-30 (3 overlap)
- 0.0 overlap correctly produces non-overlapping chunks

### Changed
- Default chunking behavior for PDFs now uses 30% overlap
- CVE processing unchanged (still uses non-overlapping chunks)

## [2025-01] Embedding Optimization & File Format Support

### Added (build_embeddings.py)
- **Three-level speed control** via `--speed` parameter:
  - `normal`: Baseline quality (float32, batch_size=32, chunk_size=10)
  - `fast`: Recommended default (float16, batch_size=64, chunk_size=10, 1.5-2x faster)
  - `fastest`: Maximum speed (float16, batch_size=128, chunk_size=20, 2-3x faster)
- **Batch encoding optimization**: Process all chunks at once instead of one-by-one (10-20x faster on GPU, 3-5x on CPU)
- **Multiple output formats** via `--extension` parameter:
  - `csv`: Text format, largest (~95 MB), slowest read, maximum compatibility
  - `pkl`: Pickle (default), balanced (~33 MB), Python-native
  - `parquet`: Parquet, smallest (~24 MB), fastest read (5-10x), requires pyarrow
  - `chroma`: Vector database (directory-based), optimized queries, no server required, best for large datasets
- **Automatic device selection**: Initializes SentenceTransformer directly on correct device (CPU/CUDA)
- **Progress reporting**: Real-time progress bar during batch encoding

### Added (validate_report.py)
- **Embedding format support** via `--extension` parameter:
  - Reads `CVEEmbeddings.{extension}` (or directory for chroma) based on parameter
  - Supports csv, pkl (default), parquet, chroma formats
  - File/directory existence check with helpful error messages
- **Format-specific loading**: Optimized readers for each format
  - Chroma: Direct vector database queries (no need to load all embeddings into memory)

### Added (extract_cve.py)
- **Verbose mode** via `--verbose` or `-v` flag:
  - Default: Shows progress bar with file count
  - Verbose: Shows detailed file-by-file logging
- **Progress bar integration** using tqdm (non-verbose mode)

### Changed
- **build_embeddings.py**: Default output format changed to `.pkl` (from `.csv`)
- **validate_report.py**: Default embedding format changed to `.pkl` (from `.csv`)
- **requirements.txt**: Added `pyarrow` for parquet support, `chromadb` for vector database support
- User input flow: Now asks for base filename without extension (for file-based formats)
- Chroma format creates directory instead of file (e.g., `cve_embeddings/` instead of `cve_embeddings.pkl`)

### Performance Impact (build_embeddings.py)
- **Batch encoding speedup**:
  - GPU (GTX 1660 Ti): 10-20x faster than one-by-one encoding
  - CPU: 3-5x faster than one-by-one encoding
- **File size reduction**:
  - pkl: 65% smaller than csv (~33 MB vs 95 MB)
  - parquet: 75% smaller than csv (~24 MB vs 95 MB)
- **Read speed improvement** (theRag.py):
  - pkl: 3-5x faster than csv
  - parquet: 5-10x faster than csv

### Technical Details
- Batch encoding uses `SentenceTransformer.encode()` with configurable `batch_size`
- Float16 precision reduces memory bandwidth by 50% with negligible quality loss
- Float16 conversion applied post-encoding via `embeddings.astype(np.float16)`
- Larger chunk sizes (20 vs 10) reduce total chunk count by ~50%
- Parquet format uses columnar storage with snappy compression

### Verified Configurations
âœ… **Tested and verified working (2025-01-13)**:
- `build_embeddings.py --speed=fast --extension=pkl` â†’ cve_embeddings.pkl (~33 MB)
- `build_embeddings.py --speed=fastest --extension=parquet` â†’ cve_embeddings.parquet (~24 MB)
- `validate_report.py --speed=fast --extension=pkl` â†’ Successfully loaded and processed
- `validate_report.py --speed=fastest --extension=parquet` â†’ Successfully loaded and processed
- All combinations tested on CUDA 11.8 (GTX 1660 Ti) without issues

âœ… **Chroma integration (2025-01-14)**:
- Added chromadb to requirements.txt
- `build_embeddings.py --extension=chroma` â†’ Creates cve_embeddings/ directory with vector database
- `validate_report.py --extension=chroma` â†’ Direct query from Chroma database (no memory loading)
- Persistent client mode (no server required)

### Bug Fixes
- Fixed `SentenceTransformer.encode()` dtype parameter issue (not supported)
- Changed to post-encoding conversion: `embeddings.astype(np.float16)`
- Updated setup scripts to use `python -m pip` instead of direct `pip.exe` calls
  - Resolves shebang issues after venv rename (venv-* â†’ .venv-*)
  - More reliable and follows Python best practices

## [2025-01] Multi-Level Speed Optimization

### Added
- **Three-level speed control** via `--speed` parameter:
  - `normal`: Baseline with chunk-aware filtering (FP32, frequent cache clearing)
  - `fast`: Recommended default (FP16, reduced cache clearing)
  - `fastest`: Maximum speed (FP16, reduced cache, low temperature, SDPA)
- SDPA (Scaled Dot-Product Attention) support for fastest mode
- Configurable cache clearing frequency (every 1 or 3 chunks)
- Dynamic temperature control (0.3 for normal/fast, 0.1 for fastest)

### Performance Impact
- **Option 2 (CVE Validation)**:
  - Original: 34.4 minutes (2.6 tok/s)
  - normal: 4.5 minutes (7.6x faster)
  - fast: 3.1 minutes (11x faster) â­ **default**
  - fastest: 2.1 minutes (16x faster)

- **Option 1 (Summarization)** & **Option 3 (Q&A)**:
  - 30-60% faster with fast/fastest modes

### Changed
- Default speed level is now `fast` (was implicit normal)
- FP16 enabled by default for better GPU utilization
- Cache clearing optimized to every 3 chunks (from every chunk)

## [2025-01] Chunk-Aware CVE Filtering

### Added
- Intelligent CVE filtering in Option 2 (CVE Validation)
- Per-chunk CVE extraction using regex
- Dynamic system prompt generation with only relevant CVEs

### Performance Impact
- Reduced input tokens from 3,934 â†’ ~1,900 per chunk (50% reduction)
- Generation speed improved from 2.6 tok/s â†’ 15-20 tok/s (6-8x faster)
- Memory bandwidth usage reduced by 50%

### Technical Details
- Parses CVE descriptions into dictionary for O(1) lookup
- Extracts CVEs from each chunk using `extract_cves_regex()`
- Filters CVE descriptions to only include mentioned CVEs
- 200-token overlap between chunks catches cross-references

### Changed
- `menu_option_2()` full mode now uses chunk-aware filtering
- No changes to Option 1 (Summarization) or Option 3 (Q&A)
- Demo mode unchanged (already uses truncation)

## [2025-01] CUDA Environment Updates

### Added
- CUDA 12.4 support for RTX 4060+ GPUs
- Setup script: `scripts/setup-cuda124.ps1`
- Virtual environment: `.venv-cuda124`
- CUDA migration guide: Documented upgrade path from 12.1 â†’ 12.4

### Changed
- Updated CLAUDE.md with CUDA 12.4 as recommended version
- Renamed CUDA 12.1 references to CUDA 11.8 for GTX 1660 Ti compatibility
- Corrected GPU references (GTX 1650 â†’ GTX 1660 Ti in documentation)

### Performance
- RTX 4060: 20-40x faster than CPU
- GTX 1660 Ti: 10-20x faster than CPU

## [2024-12] CVE Schema Unification

### Added
- Flexible schema support via `--schema` parameter:
  - `v5`: CVE 5.0 schema only (fastest)
  - `v4`: CVE 4.0 schema only
  - `all`: V5â†’V4 fallback (default, backward compatible)
- Auto-detection of v4/v5 JSON schema formats
- Unified field extraction for both schemas

### Changed
- `load_cve_record()` now supports schema parameter
- `extract_cve_fields()` detects schema automatically
- `extractCVE.py` supports schema selection and deduplication
- Default behavior: Try V5 first, fallback to V4 if not found

### Migration
- Renamed `extractCVE4.py` â†’ `extractCVE.py`
- Removed hardcoded year references
- Updated all CVE list paths to support both schemas

## [2024-12] File Organization

### Added
- `ARCHITECTURE.md`: Technical details and system architecture (contains optimization guide)
- `PROGRESS.md`: This file, tracking completed changes and upcoming features

### Changed
- Renamed `cveEmbeddings.csv` â†’ `CVEEmbeddings.csv` for naming consistency
- Standardized virtual environment naming: `venv-*` â†’ `.venv-*` (dot prefix)
- All setup scripts now create dot-prefixed venvs

### Removed
- Intermediate optimization files (theRagFast.py, theRagDebug.py, theRag_original.py)
- Redundant README files (DEBUG_README.md, FAST_README.md, SPEED_LEVELS_README.md)
- CUDA_MIGRATION.md (content merged into ARCHITECTURE.md)

### Git Changes
- Updated .gitignore to support both `venv-*` and `.venv-*` patterns
- Consolidated documentation structure

## [2024-11] Core Optimizations

### Added
- Direct regex CVE extraction (no LLM overhead)
- Global SentenceTransformer initialization (loaded once at startup)
- Unified `torch.no_grad()` wrapper for all inference calls
- Context length limits for missing CVE processing

### Performance Impact
- CVE extraction: Instant (was 10-30 seconds with LLM)
- Memory: Reduced by initializing SentenceTransformer once
- Generation efficiency: All calls use no_grad context

### Changed
- CVE extraction pattern: `CVE-\d{4}-\d{4,7}`
- Missing CVE context window: 1500 chars (full mode), 500 chars (demo mode)
- All `model.generate()` calls include `pad_token_id=tokenizer.eos_token_id`

## [2024-11] Dual-Mode Architecture

### Added
- **Demo mode** (`--mode=demo`): Memory-optimized for limited hardware
  - First 10 pages only
  - Text truncation (1000-2000 chars)
  - FP16 precision
  - Limited embedding rows (1000)
  - Token generation: 64-256 tokens

- **Full mode** (default): Complete feature set
  - All pages processed
  - Intelligent chunking (1500 tokens, 200 overlap)
  - Auto precision
  - Complete embedding database
  - Token generation: 150-700 tokens

### Performance
- Demo mode: ~4-6 GB RAM / ~3.5-4 GB VRAM
- Full mode: ~6-8 GB RAM / ~4-5 GB VRAM

## Project History

### Initial Release [2024-10]
- RAG-based CVE validation system
- Llama 3.2-1B-Instruct integration
- SentenceTransformer embeddings (all-mpnet-base-v2)
- Three analysis options: Summarize, Validate CVE, Q&A
- CVE JSON feed support (V4 schema)
- PyMuPDF text extraction
- CPU and CUDA support

### Repository Structure
```
RAG_LLM_CVE/
â”œâ”€â”€ validate_report.py     # Main application (optimized)
â”œâ”€â”€ build_embeddings.py    # Generate embedding database
â”œâ”€â”€ extract_cve.py         # Export CVE descriptions (optional)
â”œâ”€â”€ cleanup_cache.py       # Clean model cache
â”œâ”€â”€ CLAUDE.md              # Project documentation and user guide
â”œâ”€â”€ ARCHITECTURE.md        # Technical details and system architecture
â”œâ”€â”€ PROGRESS.md            # This file (completed changes and upcoming features)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ scripts/               # Environment setup scripts
â”‚   â””â”€â”€ windows/           # Windows PowerShell scripts
â”‚       â”œâ”€â”€ Setup-CPU.ps1
â”‚       â”œâ”€â”€ Setup-CUDA118.ps1
â”‚       â””â”€â”€ Setup-CUDA124.ps1
â””â”€â”€ .venv-*/               # Virtual environments (gitignored)
```

## [2025-01-18] Phase 2 LangChain Query Fix

### Fixed
- âœ… **LLM response quality issues** (Previously High Priority)
  - **Root Cause Identified**: Commit `1bf9c72` (hybrid search) introduced inconsistent query paths
    - **Path A** (CVE ID exact match): Bypassed conversation history, used simple prompt
    - **Path B** (Semantic search): Used ConversationalRetrievalChain with incompatible default prompts
  - **Problems**:
    1. Frequent "I don't know" responses despite relevant KB content
    2. Missing conversation history in CVE ID queries
    3. LangChain default prompts didn't match Llama 3.2 format
    4. Occasional hangs from chain execution issues

### Changed (rag/langchain_impl.py)
- **Refactored query() method** to use unified approach (consistent with pure_python.py):
  - Single code path for all queries (no more branching)
  - Always includes conversation history from memory
  - Explicit system prompt with context injection
  - Direct LLM invocation with properly formatted messages
- **Added _hybrid_search_unified()** method:
  - Mirrors pure_python.py implementation
  - CVE ID exact match â†’ semantic search fallback
  - Consistent retrieval behavior across Phase 1 and Phase 2
- **Added _format_messages_for_llama()** helper:
  - Uses tokenizer.apply_chat_template() when available
  - Fallback to manual formatting for robustness
  - Ensures Llama 3.2 compatibility
- **Manual memory management**:
  - Explicitly adds HumanMessage/AIMessage to memory.chat_memory
  - No longer relies on ConversationalRetrievalChain for history

### Technical Details
- **Before (broken)**:
  ```python
  # Two different code paths
  if cve_ids:
      response = self.llm(simple_prompt)  # No history!
  else:
      result = self.qa_chain({"question": question})  # Wrong prompt template
  ```
- **After (fixed)**:
  ```python
  # Unified approach
  context = self._hybrid_search_unified(question)
  messages = [system_prompt, *history, user_question]
  formatted = self._format_messages_for_llama(messages)
  response = self.llm(formatted)
  self.memory.add_messages(question, response)
  ```

### Testing
âœ… **Import verification** (2025-01-18):
- All methods exist and have correct signatures
- Docstrings updated to reference unified approach
- No syntax errors

### Impact
- **Phase 1 (pure_python.py)**: No changes (already working correctly)
- **Phase 2 (langchain_impl.py)**: Now behaves consistently with Phase 1
- **ConversationalRetrievalChain**: No longer used in query() method
  - Still initialized for backward compatibility
  - May be used in future for other features
- **Expected improvements**:
  - More accurate responses with proper context
  - Consistent conversation history across all query types
  - No more "I don't know" for valid KB content
  - Reduced hangs from simplified execution path

### Validation
âœ… **Real-world testing confirmed** (2025-01-18):
- User reported issue resolved
- Phase 2 now provides correct responses
- No more "I don't know" errors for valid KB content
- Conversation history working correctly

## Known Issues

### Phase 2: LangChain Web UI (web_ui_langchain.py)
- **No known critical issues** (as of 2025-01-18)
  - LLM response quality issues have been fixed and validated
  - Both Phase 1 and Phase 2 provide comparable performance

## Upcoming Features

### [PLANNED] Multi-file Conversation Context (High Priority)

**Status**: Detailed implementation plan completed (2025-01-18)

#### Overview
Enable chat interface to retain multiple uploaded files across conversation, allowing users to ask questions that reference multiple documents simultaneously.

#### Use Case
User uploads `report_A.pdf` and asks questions about it. Then uploads `report_B.pdf` but still needs to reference information from `report_A.pdf` for comparison or cross-referencing.

#### Architecture Decision

**Selected Strategy**: Session-scoped Chroma collection
- âœ… Supports large files (no memory limits)
- âœ… Efficient vector search (repeatable queries)
- âœ… Automatic cleanup on session end
- âœ… Consistent with existing Chroma architecture

**Rejected Alternatives**:
- âŒ In-memory embeddings: High memory usage, problematic for large files
- âŒ Permanent Chroma: Complex cleanup logic, unnecessary persistence

#### System Architecture

```
Session Files (Temporary)          Permanent Knowledge Base
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ session_abc123/     â”‚           â”‚ cve_embeddings/     â”‚
â”‚ â”œâ”€ report_A.pdf     â”‚           â”‚ â”œâ”€ CVE data         â”‚
â”‚ â”œâ”€ report_B.pdf     â”‚           â”‚ â””â”€ Permanent docs   â”‚
â”‚ â””â”€ report_C.pdf     â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
         â”‚                                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
           Dual-Source Retrieval
           (Merge & Rank Results)
                    â”‚
                    â–¼
              LLM with Context
           (File provenance tracked)
```

#### Implementation Plan (4 PRs)

**PR 1: Core Infrastructure** (`core/session_manager.py`)
- [ ] Create `SessionManager` class
  - `__init__(session_id)`: Initialize session with unique ID
  - `add_file(file_path)`: Process and embed single file
  - `remove_file(file_name)`: Remove file from session
  - `query(question, top_k)`: Query session collections
  - `cleanup()`: Delete session collection and temp files
- [ ] File lifecycle management:
  - Upload â†’ temp_uploads/session_{id}/
  - Extract text â†’ split sentences â†’ generate embeddings
  - Store in Chroma collection `session_{id}`
  - Delete on session end or manual removal
- [ ] Unit tests for session operations

**PR 2: RAG Integration** (`rag/pure_python.py`, `rag/langchain_impl.py`)
- [ ] Add `session_manager` parameter to `query()` methods
- [ ] Implement `_merge_results()`: Combine permanent KB + session results
- [ ] Implement `_build_prompt_with_sources()`: Format context with file attribution
- [ ] Dual-source retrieval:
  - Query permanent KB (top_k=3)
  - Query session files (top_k=2, if session_manager exists)
  - Merge and rank by similarity score
  - Format: `"From {source}: {text}"`
- [ ] Integration tests for dual-source queries

**PR 3: Web UI Phase 1** (`web/web_ui.py`)
- [ ] Global state changes:
  - `chat_uploaded_files = []` (list instead of single file)
  - `session_manager = SessionManager(uuid)` (initialized on load)
- [ ] File upload handlers:
  - `handle_chat_file_upload()`: Add file to session (async processing)
  - `handle_remove_file()`: Remove individual file
  - `format_file_list()`: Display HTML chips `[ğŸ“„ filename âœ…] [ğŸ—‘ï¸]`
- [ ] UI layout updates:
  - File list display above chat input
  - Individual remove buttons per file
  - Status indicators (uploading/processing/ready/error)
- [ ] Session cleanup on page reload
- [ ] UI/UX tests

**PR 4: Web UI Phase 2** (`web/web_ui_langchain.py`)
- [ ] Mirror Phase 1 implementation
- [ ] Maintain behavioral consistency
- [ ] End-to-end tests
- [ ] Performance benchmarks

#### Data Structures

**File Info Object**:
```python
{
    "name": "report_A.pdf",
    "path": "/temp_uploads/session_abc123/report_A.pdf",
    "status": "ready",  # uploading | processing | ready | error
    "chunks": 150,
    "added_date": "2025-01-18T12:34:56",
    "error": None  # or error message
}
```

**Retrieval Result Object**:
```python
{
    "text": "CVE-2024-1234 affects...",
    "source": "report_A.pdf",  # or "Knowledge Base"
    "source_type": "session",  # or "permanent"
    "score": 0.85,
    "metadata": {...}
}
```

#### Testing Strategy

**Test Cases**:
1. **Single file**: Upload 1 PDF, verify backward compatibility
2. **Multi-file**: Upload 3 PDFs, test cross-reference queries
3. **File removal**: Remove middle file, verify others unaffected
4. **Session cleanup**: Reload page, verify temp files deleted
5. **Large files**: Test with 10+ MB PDFs, monitor memory
6. **Concurrency**: Multiple browser tabs don't interfere

#### Performance Impact

**Estimates**:
- Query time: +30-50% (dual-source retrieval + merging)
- Memory: +100-500 MB per active session
- Storage: Temporary files (auto-cleanup)

**Optimizations**:
- Parallel queries to both sources
- Caching for frequently accessed files
- Lazy loading (embeddings generated on-demand)

#### Constraints & Limits

**Limits** (to prevent resource exhaustion):
- Maximum 5 files per session
- Maximum 10 MB per file
- Session timeout: 1 hour of inactivity

**Cleanup**:
- Automatic cleanup on session end (page reload)
- Periodic cleanup of orphaned collections (cronjob)
- Error handling with detailed logging

#### Expected Benefits

- âœ… Natural multi-document conversations
- âœ… Cross-reference analysis between reports
- âœ… Compare multiple threat intelligence documents
- âœ… No need to re-upload files for follow-up questions

#### Implementation Checklist

**PR 1: Core Infrastructure** (Estimated: 4-6 hours)
- [ ] Create `core/session_manager.py`
  - [ ] Import dependencies (chromadb, config, embeddings, pdf_processor)
  - [ ] Define `SessionManager` class with `__init__(session_id)`
  - [ ] Implement `add_file(file_path)`:
    - [ ] Validate file size (<=10 MB)
    - [ ] Extract text using PDFProcessor
    - [ ] Split into chunks with overlap
    - [ ] Generate embeddings using EmbeddingModel
    - [ ] Store in Chroma collection `session_{id}`
    - [ ] Return file info object
  - [ ] Implement `remove_file(file_name)`:
    - [ ] Query Chroma for documents with matching source
    - [ ] Delete documents by IDs
    - [ ] Update internal file tracking
  - [ ] Implement `query(question, top_k=5)`:
    - [ ] Generate query embedding
    - [ ] Query Chroma collection
    - [ ] Format results with file attribution
    - [ ] Return list of result objects
  - [ ] Implement `list_files()`: Return current session files
  - [ ] Implement `cleanup()`:
    - [ ] Delete Chroma collection
    - [ ] Delete temp files from disk
  - [ ] Add comprehensive docstrings
- [ ] Create `tests/test_session_manager.py`
  - [ ] Test session initialization
  - [ ] Test file upload with sample PDF
  - [ ] Test file removal
  - [ ] Test query retrieval
  - [ ] Test cleanup
  - [ ] Test file size limit enforcement
  - [ ] Test max files limit enforcement
- [ ] Update `config.py`:
  - [ ] Add `SESSION_MAX_FILES=5`
  - [ ] Add `SESSION_MAX_FILE_SIZE_MB=10`
  - [ ] Add `SESSION_TIMEOUT_HOURS=1`
- [ ] Update `.env.example` with session configuration
- [ ] Git commit: "Add SessionManager for multi-file conversation context"

**PR 2: RAG Integration** (Estimated: 3-4 hours)
- [ ] Update `rag/pure_python.py`:
  - [ ] Add `session_manager=None` parameter to `__init__()`
  - [ ] Add `_merge_results(kb_results, session_results)`:
    - [ ] Combine both result lists
    - [ ] Sort by similarity score (descending)
    - [ ] Return top-k merged results
  - [ ] Add `_build_prompt_with_sources(context_items)`:
    - [ ] Format each item as "From {source}: {text}"
    - [ ] Join with newlines
    - [ ] Return formatted string
  - [ ] Update `query()` method:
    - [ ] Query permanent KB (top_k=3)
    - [ ] If session_manager exists, query session files (top_k=2)
    - [ ] Call `_merge_results()` to combine
    - [ ] Call `_build_prompt_with_sources()` for context
    - [ ] Build system prompt with attributed context
    - [ ] Generate response as before
- [ ] Update `rag/langchain_impl.py`:
  - [ ] Mirror changes from pure_python.py
  - [ ] Maintain behavioral consistency
  - [ ] Use same `_merge_results()` logic
  - [ ] Use same `_build_prompt_with_sources()` logic
- [ ] Create `tests/test_rag_dual_source.py`:
  - [ ] Test query with session_manager=None (backward compatibility)
  - [ ] Test query with active session_manager
  - [ ] Test merging and ranking logic
  - [ ] Test source attribution in prompts
  - [ ] Compare Phase 1 vs Phase 2 output
- [ ] Git commit: "Integrate SessionManager into RAG systems"

**PR 3: Web UI Phase 1** (Estimated: 5-6 hours)
- [ ] Update `web/web_ui.py`:
  - [ ] Import SessionManager and uuid
  - [ ] Initialize global state:
    - [ ] `session_id = str(uuid.uuid4())`
    - [ ] `session_manager = SessionManager(session_id)`
    - [ ] `chat_uploaded_files = []` (list, not single file)
  - [ ] Create `handle_chat_file_upload(file)`:
    - [ ] Check max files limit (5)
    - [ ] Add file to session_manager
    - [ ] Append to chat_uploaded_files list
    - [ ] Return formatted file list HTML
  - [ ] Create `handle_remove_file(file_name)`:
    - [ ] Call session_manager.remove_file()
    - [ ] Remove from chat_uploaded_files list
    - [ ] Return updated file list HTML
  - [ ] Create `format_file_list()`:
    - [ ] Generate HTML chips: `[ğŸ“„ filename âœ…] [ğŸ—‘ï¸]`
    - [ ] Include status indicators
    - [ ] Return HTML string
  - [ ] Update `chat_query(message, history)`:
    - [ ] Pass session_manager to rag_system.query()
    - [ ] Handle session_manager=None gracefully
  - [ ] Update UI layout:
    - [ ] Replace single file upload with multi-file component
    - [ ] Add file list display area
    - [ ] Add remove buttons per file
    - [ ] Update upload status display
  - [ ] Add cleanup handler:
    - [ ] Call session_manager.cleanup() on demo.load
  - [ ] Update docstrings and comments
- [ ] Test Web UI manually:
  - [ ] Upload 1 file, verify display
  - [ ] Upload 2nd file, verify both persist
  - [ ] Ask cross-reference question
  - [ ] Remove 1 file, verify other unaffected
  - [ ] Reload page, verify cleanup
  - [ ] Test max files limit (try uploading 6th file)
- [ ] Git commit: "Add multi-file support to Phase 1 Web UI"

**PR 4: Web UI Phase 2** (Estimated: 3-4 hours)
- [ ] Update `web/web_ui_langchain.py`:
  - [ ] Mirror all changes from PR 3 (web_ui.py)
  - [ ] Use LangChainRAG instead of PureRAG
  - [ ] Maintain behavioral consistency
  - [ ] Keep port 7861
- [ ] Test Web UI manually:
  - [ ] Run same test cases as PR 3
  - [ ] Compare behavior with Phase 1
  - [ ] Verify LangChain memory still works
- [ ] End-to-end testing:
  - [ ] Test both UIs side-by-side
  - [ ] Upload same files to both
  - [ ] Ask same questions
  - [ ] Compare response quality
  - [ ] Benchmark query times
- [ ] Performance testing:
  - [ ] Monitor memory usage with 5 files
  - [ ] Test with large files (10 MB)
  - [ ] Measure query time increase
- [ ] Git commit: "Add multi-file support to Phase 2 Web UI"

**Final Steps**
- [ ] Update `CLAUDE.md`:
  - [ ] Move multi-file feature from "Upcoming" to "Features"
  - [ ] Add usage examples
  - [ ] Document constraints and limits
- [ ] Update `docs/ARCHITECTURE.md`:
  - [ ] Add SessionManager architecture diagram
  - [ ] Document dual-source retrieval algorithm
  - [ ] Add performance impact section
- [ ] Update `docs/PROGRESS.md`:
  - [ ] Move from "Upcoming Features" to "Added"
  - [ ] Document actual performance metrics
  - [ ] Add lessons learned
- [ ] Create demo video or screenshots
- [ ] Git commit: "Update documentation for multi-file feature"
- [ ] Final review and cleanup

**Estimated Total Time**: 15-20 hours

---

### Other Planned Optimizations
- [ ] Parallel CVE lookups (if memory permits)
- [ ] Progress bars for long-running operations
- [ ] Batch processing for multiple PDFs
- [ ] Input validation for CSV existence and format

### Under Consideration
- [ ] Web interface for easier access
- [ ] Support for additional LLM models
- [ ] Real-time CVE feed updates
- [ ] Export analysis results to JSON/HTML
- [ ] Configurable overlap ratio per document type (PDF vs CVE)

## Notes

- All performance benchmarks based on GTX 1660 Ti (6GB VRAM)
- Times may vary based on PDF size, CVE count, and hardware
- GPU acceleration provides 10-40x speedup depending on model
- Memory usage scales with PDF size and embedding database size
