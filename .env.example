# =============================================================================
# RAG_LLM_CVE Configuration Template
# =============================================================================
# Copy this file to .env and modify the values according to your environment
# Command: cp .env.example .env (Linux/Mac) or copy .env.example .env (Windows)
# Note: .env is gitignored and will not be committed to version control

# =============================================================================
# Paths Configuration
# =============================================================================

# Embedding base path (without extension)
# All embedding formats (csv, pkl, parquet, chroma) use this base path
# Default: ./embeddings/cve_embeddings (prebuilt, tracked by git)
# For development: ./cve_embeddings (working directory, not tracked)
EMBEDDING_PATH=./embeddings/cve_embeddings

# CVE JSON feed paths (V5 and V4 schemas)
CVE_V5_PATH=../cvelistV5/cves
CVE_V4_PATH=../cvelist

# CVE description export path (without extension)
# Used by extract_cve.py to export CVE descriptions
# Actual files: {CVE_DESCRIPTION_PATH}.txt or {CVE_DESCRIPTION_PATH}.jsonl
CVE_DESCRIPTION_PATH=./output/CVEDescription

# Temporary directory for uploaded files (web UI)
TEMP_UPLOAD_DIR=./temp_uploads

# =============================================================================
# Model Configuration
# =============================================================================

# Llama model name (from Hugging Face)
LLAMA_MODEL_NAME=meta-llama/Llama-3.2-1B-Instruct

# Embedding model name (from Hugging Face)
EMBEDDING_MODEL_NAME=sentence-transformers/all-mpnet-base-v2

# Model cache directory (optional, defaults to ~/.cache/huggingface)
# HF_HOME=./models_cache

# =============================================================================
# Default Parameters
# =============================================================================

# Speed level: normal, fast, fastest
DEFAULT_SPEED=fast

# Mode: demo, full
DEFAULT_MODE=full

# Schema: v5, v4, all
DEFAULT_SCHEMA=v5

# Embedding format: csv, pkl, parquet, chroma
# Note: Used by localEmbedding.py, theRag.py, and addToEmbeddings.py
# - chroma: Vector database (recommended, optimized queries)
# - pkl: Pickle format (balanced size/speed)
# - parquet: Smallest file, fastest read (requires pyarrow)
# - csv: Text format (largest, slowest, max compatibility)
DEFAULT_EMBEDDING_FORMAT=chroma

# Embedding precision: float32, float16
# Recommendation: float16 for all operations (consistency across tools)
# Note: Used by localEmbedding.py and addToEmbeddings.py
EMBEDDING_PRECISION=float16

# CVE content filter keyword (case-insensitive)
# Only CVEs containing this keyword will be processed
# Leave empty to process all CVEs
# Example: SNMP, Windows, Apache, etc.
CVE_FILTER=

# =============================================================================
# RAG Configuration
# =============================================================================

# Conversation history length (number of rounds)
CONVERSATION_HISTORY_LENGTH=20

# Top-K retrieval (number of similar chunks to retrieve)
RETRIEVAL_TOP_K=5

# Chunk size for PDF processing (number of sentences)
CHUNK_SIZE=10

# Chunk overlap ratio for PDF embeddings (0.0 = no overlap, 0.3 = 30% overlap recommended)
# Applies to PDF documents only; CVE data uses 0.0 (no overlap) for atomic descriptions
# Example: CHUNK_SIZE=10 with CHUNK_OVERLAP_RATIO=0.3 results in 3-sentence overlap
CHUNK_OVERLAP_RATIO=0.3

# Batch size for embedding generation
EMBEDDING_BATCH_SIZE=64

# =============================================================================
# Web UI Configuration
# =============================================================================

# Gradio server ports
GRADIO_SERVER_PORT=7860              # webUI.py (Pure Python)
GRADIO_SERVER_PORT_LANGCHAIN=7861    # webUILangChain.py (LangChain)

# Share link (set to True to generate public gradio.live link)
GRADIO_SHARE=False

# Server name (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
GRADIO_SERVER_NAME=127.0.0.1

# Maximum file upload size in MB
MAX_FILE_UPLOAD_SIZE_MB=50

# =============================================================================
# Session Management Configuration
# =============================================================================

# Maximum number of files per session (1-10)
SESSION_MAX_FILES=5

# Maximum file size per upload in MB (1-50)
SESSION_MAX_FILE_SIZE_MB=10

# Session timeout in hours
SESSION_TIMEOUT_HOURS=1

# Auto-embed uploaded files (backward compatibility control)
# True: Uploaded files are automatically embedded and searchable in RAG queries (multi-file context, requires temp_uploads/ directory)
# False: Uploaded files only used for special commands like summarize/validate (resource-efficient, no session database)
# Recommendation: False for resource efficiency (avoids temp_uploads/ directory creation), True only if multi-file conversation context needed
ENABLE_SESSION_AUTO_EMBED=False

# =============================================================================
# Advanced Configuration
# =============================================================================

# Enable verbose logging (True/False)
VERBOSE_LOGGING=False

# CUDA device index (0 for first GPU, -1 for CPU)
# Leave empty for auto-detection
CUDA_DEVICE=

# Maximum embedding rows to load (None for all, useful for testing)
MAX_EMBEDDING_ROWS=

# Temperature for LLM generation
LLM_TEMPERATURE=0.3

# Top-p for LLM generation
LLM_TOP_P=0.9

# =============================================================================
# Summary Configuration
# =============================================================================

# Chunk size for summarization (in tokens, more accurate than chars)
# Default: 1500 tokens per chunk
SUMMARY_CHUNK_TOKENS=1500

# Overlap between chunks (in tokens, prevents info loss at boundaries)
# Default: 200 tokens overlap
SUMMARY_CHUNK_OVERLAP_TOKENS=200

# Tokens generated per chunk during first-stage summarization
# Increased to 300 for more detailed summaries
SUMMARY_TOKENS_PER_CHUNK=300

# Tokens for final second-stage summary (condensing all chunk summaries)
# Increased to 600 for more comprehensive summaries
SUMMARY_FINAL_TOKENS=600

# Character threshold to trigger chunking (documents shorter than this use single-pass)
# Default: 3000 characters
SUMMARY_CHUNK_THRESHOLD_CHARS=3000

# Enable second-stage summarization (condenses multiple chunk summaries into one)
# Set to True for concise output, False to see all section summaries
SUMMARY_ENABLE_SECOND_STAGE=True

# =============================================================================
# Validation Configuration
# =============================================================================

# Chunk size for CVE validation (in tokens)
# Default: 1500 tokens per chunk (same as summary for consistency)
VALIDATION_CHUNK_TOKENS=1500

# Overlap between chunks (in tokens)
# Default: 200 tokens overlap
VALIDATION_CHUNK_OVERLAP_TOKENS=200

# Tokens generated per chunk during validation
# Increased to 500 tokens per chunk for more detailed analysis
VALIDATION_TOKENS_PER_CHUNK=500

# Character threshold to trigger chunking (documents shorter than this use single-pass)
# Default: 3000 characters
VALIDATION_CHUNK_THRESHOLD_CHARS=3000

# Enable chunk-aware CVE filtering (7-10x speedup)
# When True, only sends relevant CVE descriptions to each chunk
# Set to False to send all CVE descriptions to every chunk (slower, more context)
VALIDATION_ENABLE_CVE_FILTERING=True

# Tokens for final second-stage validation summary (consolidating all chunk results)
# Increased to 800 tokens for comprehensive validation reports
VALIDATION_FINAL_TOKENS=800

# Enable second-stage validation (consolidates chunk results into final report)
# When True, creates unified validation report with deduplication
# When False, shows all chunk-level validations separately
VALIDATION_ENABLE_SECOND_STAGE=True

# =============================================================================
# Q&A Configuration (Answering Questions About Reports)
# =============================================================================

# Chunk size for Q&A (in tokens)
# Default: 1500 tokens per chunk (same as summary/validation for consistency)
QA_CHUNK_TOKENS=1500

# Overlap between chunks (in tokens)
# Default: 200 tokens overlap (prevents info loss at boundaries)
QA_CHUNK_OVERLAP_TOKENS=200

# Tokens generated per chunk during Q&A
# Increased to 500 tokens per chunk for more complete answers
QA_TOKENS_PER_CHUNK=500

# Character threshold to trigger chunking (documents shorter than this use single-pass)
# Default: 3000 characters
QA_CHUNK_THRESHOLD_CHARS=3000

# Tokens for final second-stage Q&A (consolidating all chunk answers)
# Increased to 1000 tokens for detailed comprehensive answers
QA_FINAL_TOKENS=1000

# Enable second-stage Q&A (consolidates chunk answers into final response)
# When True, combines all chunk answers into coherent final answer
# When False, returns all chunk-level answers separately (may be redundant)
QA_ENABLE_SECOND_STAGE=True
